---
layout: post
title: cnn
---

## 7 CNN

![image-20210802212630226](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210802212630226.png)

### 7.1 用alexnet

![img](file:///C:/Users/theda/AppData/Local/Temp/enhtmlclip/Image(1).png)

<center>书上的Alex net源码结果</center>

![image-20210802185230983](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210802185230983.png)

<center>Alex net简化后<center/>

```python
net = nn.Sequential(
  # 224 
  nn.Conv2d(1,96,11,stride=4,padding=1),# padding为什么等一
  nn.ReLU(),
  # 226-11 / 4 + 1 = 54
  nn.MaxPool2d(kernel_size=3,stride=2),
  # 54-3 / 2 + 1 = 26
  nn.Conv2d(96,256,kernel_size=5,padding=2),
  nn.ReLU(),
  # (26+4-5)/1 +1 = 26
  nn.MaxPool2d(3,2),
  # 26-3 / 2 + 1 =  12
  nn.Conv2d(256,384,kernel_size=3,padding=1),
  nn.ReLU(),
  # 14-3 + 1 = 12
  # nn.Conv2d(384,384,3,padding=1), #此举并不改变size
  # nn.ReLU(),
  # 12
  nn.Conv2d(384,256,3,padding=1),
  nn.ReLU(),
  # 12
  nn.MaxPool2d(3,2),nn.Flatten(),
  # 5
  # 6400 ?= 5*5*256 == 6400 right!!!!
  nn.Linear(6400,4096),nn.ReLU(),nn.Dropout(),
  # nn.Linear(4096,4096),nn.ReLU(),nn.Dropout(), # 鸡儿用???
  nn.Linear(4096,10)
)
#      1组 1channel height length
X=torch.randn(1,  1,  224,  224)
for layer in net:
  X = layer(X)
  print(layer.__class__.__name__,' shape is ', X.shape)
batch_size = 128
train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size,resize=224)

lr,num_epochs=0.01,10
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())

```

![image-20210802213044011](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210802213044011.png)

<center>vgg block 多了很多CNN所以慢但是好<center/>

### 7.2 用block

![image-20210812225104955](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210812225104955.png)

**vgg block**

1）CNN块格局  包含一个CNN层，一个ReLU，一个MAXPooling

##### VGG network

**网络大致架构**： CNN + 全连接块（两者中间的nn.Flatten不能忘

**网络输入tensor的维度** ( 1 (样例数量), 1 (channel) ,224 (长宽), 224) ->example, channel, height, and width

VGG网络更加compact,复用

谈网络层数时候,Flatten,ReLU(由于激活函数没有需要学习训练得到的参数),

Dropout,( MaxPooling应该也**不算**层???)都不算是层 (**只有包含神经元的才算**)



### 7.3 Network in Network

**Insight ** :  use an MLP on the channels for each pixel separately

6.4介绍1X1卷积核作用(1X1 作用是在不同的channel之间建立连接,**相当于不同channel间的MLP**)

**NIN的channel与Alex net相同，但是maxpool层有overlap**

block 架构 (一个正常Conv2d，ReLU，两个1X1的卷积层做MLP（ReLU2个））

##### 本文提出了global average pooling

其作用是代替MLP简化模型,防止过拟合,

其对feature map**求平均直接得到最后的特征值**,减少层数. 下面两者图featuremap寓意不同

![image-20210806224544235](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210806224544235.png)



```python
def nin_block(in_channel, out_channel, kernel_size, strides, padding):
  return nn.Sequential(
      nn.Conv2d(in_channel, out_channel, kernel_size, strides, padding),
      nn.ReLU(),
      nn.Conv2d(out_channel, out_channel, 1),
      nn.ReLU(),
      nn.Conv2d(out_channel, out_channel, 1),
      nn.ReLU()
  )

net = nn.Sequential(
    nin_block(1, 96, 11, 4, 0),
    nn.MaxPool2d(3,2),
    nin_block(96,256,5,1,2),
    nn.MaxPool2d(3, 2),
    nin_block(256,384,3,1,1),
    nn.MaxPool2d(3,2),
    nn.Dropout(),
    nin_block(384,10,3,1,1),
    nn.AdaptiveAvgPool2d((1,1)),
    nn.Flatten()
)
X = torch.randn(1,1,224,224)
for layer in net:
  X = layer(X)
  print(layer.__class__.__name__, ' shape is ', X.shape)

lr, num_epochs, batch_size = 0.1, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```



### 7.4 并行连接的网络

为了解决多大的kernel size才是最合适的,采用多种size,最后拼接一哈

![image-20210806184154632](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210806184154632.png)

<center>使用特定的padding 使得最后4通道size相同,超参数是channel数量<center/>

![image-20210806233248067](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210806233248067.png)

其中比较复杂的是channel的设置, 作者是通过在dataset上大量实验得到的最佳值,*没有设计思想吗???*

判断所能接受的图片的最小size时,可用二分法,当通过多层CNN**图片size如果小于kernel size**,则出错.



ex3: nin,lenet通过使用global average pool避免了MLP层,减少parameter size,还用了1X1的cnn代替..

### 7.5 batch normalization

![image-20210807190041815](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210807190041815.png)
$$
\gamma和\beta都是需要学习的参数,而不是固有的
  ->这俩参数作用???好像是使得中间layer的值不会发散,通过这俩收缩到指定均值
$$

#### 7.5.1 它的主要作用减少收敛时间

​	回想一下在预测房价的时候，对输入数据进行normalize到0--1之间，这个normal很关键

​	多个layer之间，以及一个layer内的neuron的输出**范围**差异很大（即vary，变量的值的漂移可能影响收敛。

​	如一个层的输出是另一层的100倍，它的每次optim步幅很大，需对lr调整

​	batch size的选择更重要，**如果size==1**，标准化后hidden layer输入都是0，学不到任何东西，要足够大(50-100)

​	为了使方差不等0,加一个小的噪声,事实证明噪声对训练有好处(**可能这就是为啥对minibatch normal原因之一**),加快训练,避免overfit(但不知为啥)

**为什么训练时用的是minibatch的均值方差,而不是整个dataset的?**

​	可能这样会导致相同的图片,因为在不同batch而被判别不同,但是想要得到相同结果不可能,因为每过batch都会更新参数.但是model训练好之后,不存在参数变动了,此时用整个dataset的均值方差.

​	像这种train和test时行为不同,与dropout(只在**train**时起作用)类似

#### 7.5.2 实操

俩都是对输出进行BN

**全连接层:**
$$
h = \phi(BN(Wx+b))
$$
​	插入在激活函数和全连接层之间

​	X可以看成多个一维数组的stacked,数组中**每一位**的值处理后等于**所有数组中这一位**的均值方差处理	

**CNN:**

​	由于输出有多个channel,每个channel都需要BN,每个ch的俩参数都是独立的标量.

​	有m个样本的输入,输出的某个channel大小p\*q,则**对这m\*p\*q个数一起进行norm**

​	**X.mean(dim=(0,2,3))** # 注意以channel为单位处理，每个channel的所有数一起算，则其他维度压缩

**通常在训练完成后,计算整个数据集的均值方差,在预测时,固定住这俩值**	

Tensor.data每次调用，都是返回一个不同的tensor，猜测他应该是一个property装饰器,作用和detach差不多,就是**要requires_grad = False**

#### 7.5.5 调库实现

nn.BatchNorm2d(6) 6是输入的channel数目, nn.BatchNorm1d(120),全连接输出的一维长度, **即feature数**

7.5.6介绍BN work的原因

### 7.6 ResNET

![image-20210809203037641](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210809203037641.png)

只有函数集像这样层层嵌套，才能保证最后能到达f\*, 才能使得:

​	**随着层数增加,model表达的函数集范围越来越大**

如果一个layer的作用就是f(x)=x,那么有它没它model表现力差不多,只是层数多更容易拟合数据

![image-20210809204442080](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210809204442080.png)

**residual blocks**作用1 像一个普通的block一样学习出f(x),  更**容易学习到f(x)=x**这种func,即最上层weight layer参数全为0

**虚线框的输出必须要和x有同样的shape**

![image-20210809215735292](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210809215735292.png)

每个resnet block包含了几个residual block， 同样这里的channel也是玄学

==问题是这玩意为什么work？？？==

​		Inputs can forward propagate faster through the residual connections across layers.

### 7.7 densely connected network

 ResNet主要是将 f(x)=x+g(x) 即将f拆成线性的x 和 非线性的g(x)

![image-20210810085216866](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210810085216866.png)

​	右边densenet不用于左边在最后是**concatenate**而不是addition

![image-20210810091756960](C:\Users\theda\AppData\Roaming\Typora\typora-user-images\image-20210810091756960.png)

Dense Blocks就是类似resblock的,只是架构(conv BN activate)顺序不太一样(7.6练习),其中每一层CNN的输出(**concatenate后**)当作下一层CNN的输入,如此套娃

​	发现他们除了这个小结构创新,net大致架构类似的,上来一个CNN maxpool提取一下,加几个block,最后feature用global pooling layer转成相同数目的向量, 在加个MLP转一下数目

Transition Layers为了减少channel数,以及**长宽减半**

